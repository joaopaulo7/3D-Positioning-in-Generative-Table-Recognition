{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2977ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "PUBTABLES_ANN = '../../pubtables-1m/Donut_Annotations_PT/minimal/Val/'\n",
    "PUBTABLES_IMG = '../../pubtables-1m/PubTables-1M-Structure/Val/'\n",
    "\n",
    "PUBTAB_ANN = '../../pubtabnet/anns/train/'\n",
    "PUBTAB_IMG = '../../pubtabnet/imgs/train/'\n",
    "\n",
    "ANN_PATH = PUBTAB_ANN\n",
    "IMAGE_PATH = PUBTAB_IMG\n",
    "IMG_FORMAT = '.png'\n",
    "\n",
    "with open(ANN_PATH[:-1] + '_trunc_filelist.json') as file:\n",
    "    json_list = file.read().splitlines()\n",
    "    \n",
    "json_list = [item for item in json_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef480573",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_list = []\n",
    "\n",
    "for json_item in json_list:\n",
    "    aux_list.append(json_item[:-5])\n",
    "\n",
    "json_list = aux_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c72267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PMC4841970_011_00',\n",
       " 'PMC3163532_003_00',\n",
       " 'PMC5397534_007_00',\n",
       " 'PMC2996406_002_00',\n",
       " 'PMC3492782_004_00',\n",
       " 'PMC5094138_005_00',\n",
       " 'PMC4112620_005_00',\n",
       " 'PMC3880972_009_01',\n",
       " 'PMC3759865_002_00',\n",
       " 'PMC3588668_002_01']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ae6ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 18:34:31.377463: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 18:34:31.377487: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 18:34:31.377505: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 18:34:32.027642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from transformers import VisionEncoderDecoderConfig\n",
    "\n",
    "config = VisionEncoderDecoderConfig.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "image_size = [750, 750]\n",
    "max_length = 1400#config.decoder.max_position_embeddings\n",
    "\n",
    "\n",
    "config.encoder.image_size = image_size\n",
    "\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\", config=config, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d663c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartLearnedPositionalEmbedding(1538, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.model.decoder.embed_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7713b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self,  max_len: int, d_model: int, dropout: float = 0.05):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.offset = 2\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.pos_enc = pe.squeeze(1)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n",
    "        \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n",
    "\n",
    "        bsz, seq_len = input_ids.shape[:2]\n",
    "        \n",
    "        pos_start = past_key_values_length\n",
    "        pos_end = past_key_values_length+seq_len\n",
    "        \n",
    "        positions = self.pos_enc[pos_start:pos_end].expand(bsz, -1, -1)\n",
    "        \n",
    "        return self.dropout(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50455328",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc = PositionalEncoding(10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9d349ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder.model.decoder.embed_positions = PositionalEncoding(4098, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74f69b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(57559, 1024)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AddedToken\n",
    "\n",
    "new_tokens = [\"<table_extraction>\", \"<table>\", \"<row>\", \"<cell>\", \"<row_and_col_header>\", \"<row_header>\", \"<col_header>\"]\n",
    "new_tokens += [\"1\", \"≥\",\"≤\"]\n",
    "new_tokens += [\"<b>\", \"</b>\", \"<i>\", \"</i>\", \"<sup>\", \"</sup>\",\"<sub>\", \"</sub>\"]\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            new_tokens.append(\"<span_type=0\" + str(i) + str(j) + str(k) + \">\")\n",
    "            new_tokens.append(\"<span_type=1\" + str(i) + str(j) + str(k) + \">\")\n",
    "\n",
    "new_tokens = [AddedToken(tag, rstrip = True, lstrip=True, normalized=False) for tag in new_tokens]\n",
    "\n",
    "processor.tokenizer.add_tokens(new_tokens, special_tokens = False)\n",
    "model.decoder.resize_token_embeddings(len(processor.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63430609",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.image_processor.size = image_size[::-1] # should be (width, height)\n",
    "processor.image_processor.do_align_long_axis = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c74cf64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"msg.json\", 'w') as out:\n",
    "        json.dump({'outputs': []}, out, ensure_ascii=False, indent=4)\n",
    "\n",
    "def write_msg(msg):\n",
    "    with open(\"msg.json\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    with open(\"msg.json\", 'w') as out:\n",
    "        json_data['outputs'].append(msg)\n",
    "        json.dump(json_data, out, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07bba3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cel2token(cell):\n",
    "    if cell['span_type'][10:] != '0000':\n",
    "        sequence = \"<\" + cell['span_type'] + \">\"\n",
    "    else:\n",
    "        sequence = \"\"\n",
    "    \n",
    "    if cell['content_holder']:\n",
    "        if cell['row_header'] and cell['col_header']:\n",
    "            sequence += \"<row_and_col_header>\"\n",
    "        elif cell['col_header']:\n",
    "            sequence += \"<col_header>\"\n",
    "        elif cell['row_header']:\n",
    "            sequence += \"<row_header>\"\n",
    "        else:\n",
    "            sequence += \"<cell>\"\n",
    "        sequence += cell['content']\n",
    "        \n",
    "    return sequence\n",
    "\n",
    "def row2token(row):\n",
    "    sequence = \"<row>\"\n",
    "    for cell in row:\n",
    "        sequence += cel2token(cell)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "\n",
    "def table2token(table):\n",
    "    sequence = \"<table>\"\n",
    "    for row in table:\n",
    "        sequence += row2token(row)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "\n",
    "def json2token(json):\n",
    "    sequence = \"\"\n",
    "    if('tables' in json):\n",
    "        for table in json['tables']:\n",
    "            sequence += table2token(table)\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20d32c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class DonutTableDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        annotations,\n",
    "        image_size,\n",
    "        max_length,\n",
    "        shuffle = True,\n",
    "        split = \"train\",\n",
    "        ignore_id = -100,\n",
    "        prompt_end_token = None,\n",
    "    ):            \n",
    "        self.annotations = annotations\n",
    "        \n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.ignore_id = ignore_id\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        file_name = self.annotations[idx]\n",
    "        \n",
    "        with open(ANN_PATH + file_name + \".json\", encoding=\"utf-8\") as f:\n",
    "            annotation = json.load(f)\n",
    "        \n",
    "        image = Image.open(IMAGE_PATH + file_name + IMG_FORMAT)\n",
    "        \n",
    "        \n",
    "        # inputs\n",
    "        pixel_values = processor(image.convert(\"RGB\"), random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values.squeeze()\n",
    "        pixel_values = pixel_values.squeeze()\n",
    "        \n",
    "        target_sequence = \"<s>\"+json2token(annotation)+\"</s>\"\n",
    "        \n",
    "        input_ids = processor.tokenizer(\n",
    "            target_sequence,\n",
    "            add_special_tokens=True,\n",
    "            max_length= max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id\n",
    "        \n",
    "        \n",
    "        encoding = dict(pixel_values=pixel_values,\n",
    "                        labels=labels)\n",
    "        \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d7f93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids([\"<table_extraction>\"])[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5c2595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DonutTableDataset(json_list,\n",
    "                             max_length = max_length,\n",
    "                             image_size = image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5c6d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b29be2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load(\"model_epoch_checkpoint_.bin\")\n",
    "start_epoch = 0\n",
    "avg_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5cb981e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fddf0fc50a444c9fa5d2af85c9cbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'decoder.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss:  0.014694376945495606\n",
      "1000 Loss:  1.435754075422883\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(pixel_values\u001b[38;5;241m=\u001b[39mpixel_values, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, 1):\n",
    "    print(\"Epoch:\", epoch+1)\n",
    "    mean_loss = 0\n",
    "    mean_smpl_loss = 0 \n",
    "    model.train()\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        \n",
    "        \n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        \n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mean_loss += loss.item()   \n",
    "        mean_smpl_loss += loss.item() \n",
    "        if i % avg_size == 0:\n",
    "            print(str(i) + \" Loss: \", mean_smpl_loss/avg_size)\n",
    "            write_msg(\"batch \" + str(i) +\" loss: \"+ str(mean_smpl_loss/avg_size))\n",
    "            mean_smpl_loss = 0 \n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "             model.save_pretrained(\"../../pubtabnet/TML-lre-5-checkpoint\")\n",
    "    \n",
    "    model.save_pretrained(\"../../pubtabnet/TML-lre-5-checkpoint\")\n",
    "    print(\"Epoch's mean loss: \", mean_loss/len(train_dataloader))\n",
    "    \n",
    "    write_msg(\"Epoch checkpointed: \" + str(epoch+1) +\" \\n\"+\n",
    "              \"Epoch's mean Loss: \" + str(mean_loss/len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3289249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../../pubtabnet/model-minimal-epoch1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715dfb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(\"../../pubtabnet/Donut_PubTables_TML_Processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6cd87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2375e00",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
