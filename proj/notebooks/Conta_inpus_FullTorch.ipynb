{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2977ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "SPLIT = \"val/\"\n",
    "\n",
    "PUBTABLES = '../../pubtables-1m/Donut_Annotations_PT/minimal/'\n",
    "PUBTABNET = '../../pubtabnet/anns/'\n",
    "\n",
    "ANN_PATH = PUBTABNET\n",
    "\n",
    "json_list = os.listdir(ANN_PATH + SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ec09de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_list = []\n",
    "html_list = []\n",
    "\n",
    "for f in json_list:\n",
    "    if(f[-6:-5] == \"L\"):\n",
    "        html_list.append(f)\n",
    "    else:\n",
    "        aux_list.append(f)\n",
    "json_list = aux_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c72267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PMC3752962_006_00.json',\n",
       " 'PMC5285298_009_01.json',\n",
       " 'PMC4721713_013_01.json',\n",
       " 'PMC3189161_006_00.json',\n",
       " 'PMC5921262_006_00.json',\n",
       " 'PMC3940998_002_00.json',\n",
       " 'PMC4211402_009_00.json',\n",
       " 'PMC2796464_003_00.json',\n",
       " 'PMC4714473_005_00.json',\n",
       " 'PMC4900244_009_01.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ae6ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 19:36:32.294616: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-06 19:36:32.294642: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-06 19:36:32.294659: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-06 19:36:32.936416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DonutProcessor\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"../../pubtabnet/Donut_PubTables_TML_Processor8k\")\n",
    "\n",
    "new_tokens = [\"<table_extraction>\", \"<table>\", \"<row>\", \"<cell>\", \"<row_and_col_header>\", \"<row_header>\", \"<col_header>\"]\n",
    "new_tokens += [\"<content_row_and_col_header>\", \"<content_row_header>\", \"<content_col_header>\", \"<content>\"]\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            new_tokens.append(\"<span_type=0\" + str(i) + str(j) + str(k) + \">\")\n",
    "            new_tokens.append(\"<span_type=1\" + str(i) + str(j) + str(k) + \">\")\n",
    "\n",
    "\n",
    "processor.tokenizer.add_tokens(new_tokens, special_tokens = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f88954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DonutProcessor.from_pretrained(\"../../pubtabnet/Donut_PubTables_TML_Processor8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "690a1f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 44, 2], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer(\"<content_col_header>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c74cf64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_msg(msg):\n",
    "    with open(\"msg.json\", 'w') as out:\n",
    "        json.dump({'msg': msg}, out, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07bba3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cel2token(cell):\n",
    "    if cell['span_type'][10:] != '0000':\n",
    "        sequence = \"<\" + cell['span_type'] + \">\"\n",
    "        if cell['content_holder']:\n",
    "            if cell['row_header'] and cell['col_header']:\n",
    "                sequence += \"<content_row_and_col_header>\"\n",
    "            elif cell['col_header']:\n",
    "                sequence += \"<content_col_header>\"\n",
    "            elif cell['row_header']:\n",
    "                sequence += \"<content_row_header>\"\n",
    "            else:\n",
    "                sequence += \"<content>\"\n",
    "            sequence += cell['content']\n",
    "    else:\n",
    "        sequence = \"\"\n",
    "        if cell['content_holder']:\n",
    "            if cell['row_header'] and cell['col_header']:\n",
    "                sequence += \"<row_and_col_header>\"\n",
    "            elif cell['col_header']:\n",
    "                sequence += \"<col_header>\"\n",
    "            elif cell['row_header']:\n",
    "                sequence += \"<row_header>\"\n",
    "            else:\n",
    "                sequence += \"<cell>\"\n",
    "            sequence += cell['content']\n",
    "        \n",
    "    return sequence\n",
    "\n",
    "def row2token(row):\n",
    "    sequence = \"<row>\"\n",
    "    for cell in row:\n",
    "        sequence += cel2token(cell)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "\n",
    "def table2token(table):\n",
    "    sequence = \"<table>\"\n",
    "    for row in table:\n",
    "        sequence += row2token(row)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "\n",
    "def json2token(json):\n",
    "    sequence = \"\"\n",
    "    if('tables' in json):\n",
    "        for table in json['tables']:\n",
    "            sequence += table2token(table)\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20d32c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class DonutTableDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        annotations,\n",
    "        shuffle = True,\n",
    "        split = \"train\",\n",
    "        ignore_id = -100,\n",
    "        prompt_end_token = None,\n",
    "    ):            \n",
    "        self.annotations = annotations\n",
    "        \n",
    "        self.split = split\n",
    "        self.ignore_id = ignore_id\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        file_name = self.annotations[idx]\n",
    "        \n",
    "        with open(ANN_PATH + SPLIT + file_name, encoding=\"utf-8\") as f:\n",
    "            annotation = json.load(f)\n",
    "            \n",
    "        try:\n",
    "            target_sequence = json2token(annotation)\n",
    "        except:\n",
    "            print(file_name)\n",
    "            raise cu\n",
    "        \n",
    "        input_ids = processor.tokenizer(\n",
    "            target_sequence,\n",
    "            add_special_tokens=True,\n",
    "            max_length= 6000,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id  \n",
    "        \n",
    "        encoding = dict(labels=labels,\n",
    "                        file_name = file_name)\n",
    "        \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5c2595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DonutTableDataset(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5c6d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dee3e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dics = []\n",
    "\n",
    "def add_lens(file_name, tokens):\n",
    "    for i in range(len(tokens)):\n",
    "        j = 0\n",
    "        for token in tokens[i]:\n",
    "            j += 1\n",
    "            if(token == -100):\n",
    "                break\n",
    "\n",
    "        len_dics.append({'file': file_name[i], 'len': j})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5cb981e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adc06be880d471493763235def491ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    labels = batch[\"labels\"]\n",
    "    filename = batch[\"file_name\"]\n",
    "    add_lens(filename, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cdd3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(ANN_PATH + SPLIT[:-1] + \"_lens.json\", 'w') as out:\n",
    "    json.dump(len_dics, out, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0ebbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fa8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
