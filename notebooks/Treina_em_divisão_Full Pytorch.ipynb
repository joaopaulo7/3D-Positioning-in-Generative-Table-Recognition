{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2977ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "PUBTABLES_ANN = '../../pubtables-1m/Donut_Annotations_PT/minimal/Val/'\n",
    "PUBTABLES_IMG = '../../pubtables-1m/PubTables-1M-Structure/Val/'\n",
    "\n",
    "PUBTAB_ANN = '../../pubtabnet/anns/train/'\n",
    "PUBTAB_IMG = '../../pubtabnet/imgs/train/'\n",
    "\n",
    "ANN_PATH = PUBTAB_ANN\n",
    "IMAGE_PATH = PUBTAB_IMG\n",
    "IMG_FORMAT = '.png'\n",
    "\n",
    "with open(ANN_PATH[:-1] + '_trunc_filelist.json') as file:\n",
    "    json_list = file.read().splitlines()\n",
    "    \n",
    "json_list = [item for item in json_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef480573",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_list = []\n",
    "\n",
    "for json_item in json_list:\n",
    "    aux_list.append(json_item[:-5])\n",
    "\n",
    "json_list = aux_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c72267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PMC6051241_006_00',\n",
       " 'PMC3398398_014_00',\n",
       " 'PMC2682793_006_00',\n",
       " 'PMC6082926_006_00',\n",
       " 'PMC2809502_009_00',\n",
       " 'PMC3890642_004_00',\n",
       " 'PMC3926592_006_01',\n",
       " 'PMC548940_003_00',\n",
       " 'PMC4394591_006_00',\n",
       " 'PMC1906827_006_00']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ae6ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at naver-clova-ix/donut-base and are newly initialized because the shapes did not match:\n",
      "- decoder.model.decoder.embed_positions.weight: found shape torch.Size([1538, 1024]) in the checkpoint and torch.Size([4502, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from transformers import VisionEncoderDecoderConfig\n",
    "\n",
    "config = VisionEncoderDecoderConfig.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "image_size = [750, 750]\n",
    "max_length = 1200#config.decoder.max_position_embeddings\n",
    "\n",
    "\n",
    "config.encoder.image_size = image_size\n",
    "\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\", config=config, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f69b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(57559, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AddedToken\n",
    "\n",
    "new_tokens = [\"<table_extraction>\", \"<table>\", \"<row>\", \"<cell>\", \"<row_and_col_header>\", \"<row_header>\", \"<col_header>\"]\n",
    "new_tokens += [\"1\", \"≥\",\"≤\"]\n",
    "new_tokens += [\"<b>\", \"</b>\", \"<i>\", \"</i>\", \"<sup>\", \"</sup>\",\"<sub>\", \"</sub>\"]\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            new_tokens.append(\"<span_type=0\" + str(i) + str(j) + str(k) + \">\")\n",
    "            new_tokens.append(\"<span_type=1\" + str(i) + str(j) + str(k) + \">\")\n",
    "\n",
    "new_tokens = [AddedToken(tag, rstrip = True, lstrip=True, normalized=False) for tag in new_tokens]\n",
    "\n",
    "processor.tokenizer.add_tokens(new_tokens, special_tokens = False)\n",
    "model.decoder.resize_token_embeddings(len(processor.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c75b509a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DonutProcessor:\n",
       "- image_processor: DonutImageProcessor {\n",
       "  \"do_align_long_axis\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_pad\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"do_thumbnail\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"DonutImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"processor_class\": \"DonutProcessor\",\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 2560,\n",
       "    \"width\": 1920\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: XLMRobertaTokenizerFast(name_or_path='naver-clova-ix/donut-base', vocab_size=57522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['<s_iitcdip>', '<s_synthdog>']}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63430609",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.image_processor.size = image_size[::-1] # should be (width, height)\n",
    "processor.image_processor.do_align_long_axis = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c74cf64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"msg.json\", 'w') as out:\n",
    "        json.dump({'outputs': []}, out, ensure_ascii=False, indent=4)\n",
    "\n",
    "def write_msg(msg):\n",
    "    with open(\"msg.json\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    with open(\"msg.json\", 'w') as out:\n",
    "        json_data['outputs'].append(msg)\n",
    "        json.dump(json_data, out, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07bba3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cel2token(cell):\n",
    "    if cell['span_type'][10:] != '0000':\n",
    "        sequence = \"<\" + cell['span_type'] + \">\"\n",
    "    else:\n",
    "        sequence = \"\"\n",
    "    \n",
    "    if cell['span_type'][10:] in ['0000', '0100', '1000', '1100']:\n",
    "        if cell['row_header'] and cell['col_header']:\n",
    "            sequence += \"<row_and_col_header>\"\n",
    "        elif cell['col_header']:\n",
    "            sequence += \"<col_header>\"\n",
    "        elif cell['row_header']:\n",
    "            sequence += \"<row_header>\"\n",
    "        else:\n",
    "            sequence += \"<cell>\"\n",
    "        sequence += cell['content']\n",
    "        \n",
    "    return sequence\n",
    "\n",
    "def row2token(row):\n",
    "    sequence = \"<row>\"\n",
    "    for cell in row:\n",
    "        sequence += cel2token(cell)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "\n",
    "def table2token(table):\n",
    "    sequence = \"<table>\"\n",
    "    for row in table:\n",
    "        sequence += row2token(row)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "\n",
    "def json2token(json):\n",
    "    sequence = \"\"\n",
    "    if('tables' in json):\n",
    "        for table in json['tables']:\n",
    "            sequence += table2token(table)\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20d32c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class DonutTableDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        annotations,\n",
    "        image_size,\n",
    "        max_length,\n",
    "        shuffle = True,\n",
    "        split = \"train\",\n",
    "        ignore_id = -100,\n",
    "        prompt_end_token = None,\n",
    "    ):            \n",
    "        self.annotations = annotations\n",
    "        \n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.ignore_id = ignore_id\n",
    "        \n",
    "        self.resize = transforms.Compose([transforms.Resize(image_size)])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        file_name = self.annotations[idx]\n",
    "        \n",
    "        with open(ANN_PATH + file_name + \".json\", encoding=\"utf-8\") as f:\n",
    "            annotation = json.load(f)\n",
    "        \n",
    "        image = Image.open(IMAGE_PATH + file_name + IMG_FORMAT)\n",
    "        \n",
    "        \n",
    "        # inputs\n",
    "        pixel_values = processor(image.convert(\"RGB\"), random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values.squeeze()\n",
    "        pixel_values = pixel_values.squeeze()\n",
    "        \n",
    "        target_sequence = json2token(annotation)\n",
    "        \n",
    "        input_ids = processor.tokenizer(\n",
    "            target_sequence,\n",
    "            add_special_tokens=True,\n",
    "            max_length= max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id\n",
    "        \n",
    "        \n",
    "        encoding = dict(pixel_values=pixel_values,\n",
    "                        labels=labels)\n",
    "        \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d7f93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids([\"<table_extraction>\"])[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5c2595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DonutTableDataset(json_list,\n",
    "                             max_length = max_length,\n",
    "                             image_size = image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5c6d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b29be2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "avg_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5cb981e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c79bfd140b14aa5af867f768b38b95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss:  0.0072148447036743165\n",
      "2000 Loss:  1.888123303629458\n",
      "4000 Loss:  0.7532004588209092\n",
      "6000 Loss:  0.5901009112671017\n",
      "8000 Loss:  0.5001148815397173\n",
      "10000 Loss:  0.43627669353596865\n",
      "12000 Loss:  0.38893353652767837\n",
      "14000 Loss:  0.3666979767140001\n",
      "16000 Loss:  0.3319789090733975\n",
      "18000 Loss:  0.32398891280964015\n",
      "20000 Loss:  0.3007823115345091\n",
      "22000 Loss:  0.2860110554387793\n",
      "24000 Loss:  0.26769006344024093\n",
      "26000 Loss:  0.26157661773264407\n",
      "28000 Loss:  0.26227703138114883\n",
      "30000 Loss:  0.2412826177701354\n",
      "32000 Loss:  0.2384787759622559\n",
      "34000 Loss:  0.22835533570079133\n",
      "36000 Loss:  0.2179843672523275\n",
      "38000 Loss:  0.21644735558656975\n",
      "40000 Loss:  0.21363224599091335\n",
      "42000 Loss:  0.2031867425092496\n",
      "44000 Loss:  0.20249907360132785\n",
      "46000 Loss:  0.19889624081365764\n",
      "48000 Loss:  0.1986171723329462\n",
      "50000 Loss:  0.18461400424782187\n",
      "52000 Loss:  0.18521537876175717\n",
      "54000 Loss:  0.19738844891590998\n",
      "56000 Loss:  0.1805361046139151\n",
      "58000 Loss:  0.1739906374993734\n",
      "60000 Loss:  0.1691030085363891\n",
      "62000 Loss:  0.17699005279038102\n",
      "64000 Loss:  0.1655888730841689\n",
      "66000 Loss:  0.16113448633323424\n",
      "68000 Loss:  0.16227142270095646\n",
      "70000 Loss:  0.1611417327807285\n",
      "72000 Loss:  0.15746579883433878\n",
      "74000 Loss:  0.1604504013620317\n",
      "76000 Loss:  0.1489076957097277\n",
      "78000 Loss:  0.1515950520755723\n",
      "80000 Loss:  0.15330842269258574\n",
      "82000 Loss:  0.14864261692936998\n",
      "84000 Loss:  0.1467874419130385\n",
      "86000 Loss:  0.14044533760799094\n",
      "88000 Loss:  0.140484701089561\n",
      "90000 Loss:  0.14202433973830192\n",
      "92000 Loss:  0.14233079805690796\n",
      "94000 Loss:  0.14243576916027814\n",
      "96000 Loss:  0.13729526003217324\n",
      "98000 Loss:  0.1359775876775384\n",
      "100000 Loss:  0.13408194351382555\n",
      "102000 Loss:  0.13317851382913068\n",
      "104000 Loss:  0.12748192331334576\n",
      "106000 Loss:  0.13057062702195255\n",
      "108000 Loss:  0.12461163730034605\n",
      "110000 Loss:  0.12886375085730106\n",
      "112000 Loss:  0.12734930502227507\n",
      "114000 Loss:  0.12513119201664813\n",
      "116000 Loss:  0.12431872544065117\n",
      "118000 Loss:  0.12175902053155005\n",
      "120000 Loss:  0.1245815686690621\n",
      "Epoch's mean loss:  0.24120635422117154\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, 1):\n",
    "    print(\"Epoch:\", epoch+1)\n",
    "    mean_loss = 0\n",
    "    mean_smpl_loss = 0 \n",
    "    model.train()\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        \n",
    "        \n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        \n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mean_loss += loss.item()   \n",
    "        mean_smpl_loss += loss.item() \n",
    "        if i % avg_size == 0:\n",
    "            print(str(i) + \" Loss: \", mean_smpl_loss/avg_size)\n",
    "            write_msg(\"batch \" + str(i) +\" loss: \"+ str(mean_smpl_loss/avg_size))\n",
    "            mean_smpl_loss = 0 \n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "             model.save_pretrained(\"../../pubtabnet/TML-lre-5-checkpoint\")\n",
    "    \n",
    "    model.save_pretrained(\"../../pubtabnet/TML-lre-5-checkpoint-epoch_\"+str(epoch+1))\n",
    "    print(\"Epoch's mean loss: \", mean_loss/len(train_dataloader))\n",
    "    \n",
    "    write_msg(\"Epoch checkpointed: \" + str(epoch+1) +\" \\n\"+\n",
    "              \"Epoch's mean Loss: \" + str(mean_loss/len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3289249e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481442"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a35e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../../pubtabnet/model-minimal-epoch1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "715dfb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(\"../../pubtabnet/Donut_PubTables_TML_Processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6cd87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d68a8b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jao/testes/PT/minimal\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f427974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
